{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Py35",
      "language": "python",
      "name": "py35"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "Luke Turbert Lab1 Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNVe3UYX4gWa",
        "colab_type": "text"
      },
      "source": [
        "# **Meet NNs at their purest – the Perceptron**\n",
        "The peculiar characteristic that unites all NNs (regardless of their implementation complexity) is that they conceptually mimic the behavior of the human brain. The most basic structure we encounter when we analyze the behavior of the brain, is undoubtedly the neuron.The Perceptron is one of the first successful implementations of a neuron in the field of Artificial Intelligence (AI). Just like a neuron in the human brain, it is characterized by a layered structure, aimed at associating a result in output to certain input levels, as shown in the following diagram:\n",
        "\n",
        "![Neuron](https://github.com/behzadanksu/AISecS20/blob/master/lab1/neuron.png?raw=true)\n",
        "\n",
        "In the same way, the artificial representation of the neuron implemented through the Perceptron model is structured in such a way as to associate a given output value to one or more levels of input data:\n",
        "\n",
        "![alt text](https://github.com/behzadanksu/AISecS20/blob/master/lab1/perceptron.png?raw=true)\n",
        "\n",
        "# It's all about finding the right weight!\n",
        "\n",
        "One of the differences in the approach between the statistical models and the AI algorithms is that the algorithms implement an optimization strategy based on the iteration. At each iteration, in fact, the algorithm tries to adjust its own estimate of the values, attributing to them a greater or lesser weight depending on the cost function that we must minimize. One of the aims of the algorithm is to identify precisely an optimal weight vector to be applied to the estimated values in order to obtain reliable future predictions on unknown future data.To fully understand the power of AI algorithms applied to spam detection, we must first clarify the ideas on which tasks we should perform a spam filter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX7EB3wv3a28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute plot() inline without calling show()\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('phishing_dataset.csv')\n",
        "\n",
        "y = df.iloc[:,30]\n",
        "x = df.iloc[:,0:29]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc-S30Bm3a2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "         x, y, test_size=0.20, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2dr3V6h3a3B",
        "colab_type": "code",
        "outputId": "45a504bf-88db-4cc9-f7fc-6ec03b3abdf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "p = Perceptron()\n",
        "p.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
              "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
              "           validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgW440wg3a3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = p.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEfBW6hs3a3K",
        "colab_type": "code",
        "outputId": "9edc42a6-0d0d-4125-c282-fdc95bfffeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Misclassified samples: %d' % (y_test != y_pred).sum())\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Misclassified samples: 244\n",
            "Accuracy: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}