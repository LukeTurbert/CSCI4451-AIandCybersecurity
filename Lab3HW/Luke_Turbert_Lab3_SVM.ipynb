{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Py35",
      "language": "python",
      "name": "py35"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "Luke Turbert Lab3 SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-Cr58l43bfH",
        "colab_type": "text"
      },
      "source": [
        "# **Spam detection with SVMs**\n",
        "SVMs are an example of supervised algorithms (as well as the Perceptron), whose task is to identify the hyperplane that best separates classes of data that can be represented in a multidimensional space. It is possible, however, to identify different hyperplanes that correctly separate the data from each other; in this case, the choice falls on the hyperplane that optimizes the prefixed margin, that is, the distance between the hyperplane and the data.One of the advantages of the SVM is that the identified hyperplane is not limited to the linear model (unlike the Perceptron), as shown in the following screenshot:\n",
        "\n",
        "![alt text](http://vbehzadan.com/AISec/SVM1.png)\n",
        "\n",
        "The SVM can be considered as an extension of the Perceptron, however. While in the case of the Perceptron, our goal was to minimize classification errors, in the case of SVM, our goal instead is to maximize the margin, that is, the distance between the hyperplane and the training data closest to the hyperplane (the nearest training data is thus known as a **support vector**).\n",
        "\n",
        "**# SVM Optimization Strategy**\n",
        "Why choose the hyperplane that maximizes the margin in the first place? The reason lies in the fact that wider margins correspond to fewer classification errors, while with narrower margins we risk incurring the phenomenon known as overfitting (a real disaster that we may incur when dealing with iterative algorithms, as we will see when we will discuss verification and optimization strategies for our AI solutions).We can translate the SVM optimization strategy in mathematical terms, similar to what we have done in the case of the Perceptron (which remains our starting point). We define the condition that must be met to assure that the SVM correctly identifies the best hyperplane that separates the classes of data:\n",
        "![alt text](http://vbehzadan.com/AISec/SVM2.png)\n",
        "Here, the β constant represents the bias, while µ represents our margin (which assumes the maximum possible positive value in order to obtain the best separation between the classes of values)\n",
        "\n",
        "In practice, to the algebraic multiplication we add the value of the β bias, which allows us to obtain a value greater than or equal to zero, in the presence of values ​​that fall in the same class label (remember that y can only assume the values of ​​-1 or +1 to distinguish between the corresponding classes to which the samples belong, as we have already seen in the case of the Perceptron).\n",
        "\n",
        "At this point, the value calculated in this way is compared with the Mu margin in order to ensure that the distance between each sample and the separating hyperplane we identified (thus constituting our decision boundary) is greater or at most equal to our margin (which, as we have seen, is identified as the maximum possible positive value, in order to obtain the best separation between the classes of values).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhihg4j13v41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  \n",
        "\n",
        "df = pd.read_csv('phishing_dataset.csv')\n",
        "\n",
        "y = df.iloc[:, 30].values\n",
        "x = df.iloc[:,0:29].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_UHfm-q3v44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "         x, y, test_size=0.20, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N89i6Zm3v4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='linear', C=1.0, random_state=1)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ20UDNQ3v5E",
        "colab_type": "code",
        "outputId": "8217c5e0-c032-4ffc-fa58-79d1187d9537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Misclassified samples: %d' % (y_test != y_pred).sum())\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Misclassified samples: 121\n",
            "Accuracy: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}